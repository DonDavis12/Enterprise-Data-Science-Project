{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ciUcUw8imvx"
   },
   "source": [
    "# One run full walktrhough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R0-_tQTimv3"
   },
   "source": [
    "* Do the full walk through on the large data set\n",
    "* Refactor the source code and bring it to individual scripts\n",
    "* Ensure a full run with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pkJBSwVUimv-",
    "outputId": "26227c5f-73b8-4fbf-8cc3-36334da489a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your base path is at: Lecture_Covid_19_data_analysis-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE9I6oZ0imwC"
   },
   "source": [
    "## 1. Update all data (Data Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "id": "ShnsZRFUimwD",
    "outputId": "efbd9fd9-0676-4f4a-f441-5696a6ad646a"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdMGk7mBimwF"
   },
   "source": [
    "## 2. Process pipeline (Data Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMi0Ga1dimwG",
    "outputId": "abad471f-ddc0-4640-9cc8-670b4bbadf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 238788\n",
      " Latest date is: 2022-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def store_relational_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "    data_path=\"C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/raw/time_series_covid19_confirmed_global.csv\"\n",
    "    pd_raw=pd.read_csv(data_path)\n",
    "\n",
    "    time_idx = pd_raw.columns[4:]\n",
    "    df_plot = pd.DataFrame({\n",
    "        'date':time_idx})\n",
    "    df_input_large= pd_raw['Country/Region'].unique()\n",
    "    \n",
    "    for each in df_input_large:\n",
    "        df_plot[each] =np.array(pd_raw[pd_raw['Country/Region']==each].iloc[:,4::].sum(axis=0))\n",
    "    df = df_plot.drop('date', axis=1)\n",
    "    \n",
    "    #Merging the data set over COUNTRY for CODE column for worldmap\n",
    "    df_code = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')\n",
    "    world_raw =  pd.DataFrame({\"COUNTRY\" : df_input_large, \"Confirm cases\" :df.iloc[-1]})\n",
    "    world_con = pd.merge(world_raw, df_code, on = \"COUNTRY\").drop('GDP (BILLIONS)', axis=1)\n",
    "    world_con.to_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_CRD.csv',sep=';',index=False)\n",
    "    \n",
    "    #Continuation of data preparation \n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'COUNTRY',\n",
    "                      'Province/State':'state'})\n",
    "\n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')\n",
    "\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "\n",
    "\n",
    "    pd_relational_model_1=pd_data_base.set_index(['state','COUNTRY']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                  )\n",
    "    pd_relational_model = pd.merge(pd_relational_model_1, df_code, on = \"COUNTRY\").drop('GDP (BILLIONS)', axis=1)\n",
    "    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "    \n",
    "    pd_relational_model.to_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/20200823_COVID_relational_confirmed.csv',sep=';',index=False)\n",
    "    \n",
    "    \n",
    "    #SIR model data preparation\n",
    "    sir_plot = pd.DataFrame({\n",
    "    'date':time_idx})\n",
    "    #sir_plot.head()\n",
    "    sir_arr= pd_raw['Country/Region'].unique()\n",
    "    sir_list = sir_arr.tolist()\n",
    "    for each in sir_list:\n",
    "        sir_plot[each] =np.array(pd_raw[pd_raw['Country/Region']==each].iloc[:,4::].sum(axis=0))\n",
    "    #sir_plot.head()\n",
    "    \n",
    "    #Creating SIR plot for 100+ countries\n",
    "    sir_plot= sir_plot.drop(columns = ['Taiwan*', 'South Sudan', 'Guyana','Haiti', 'Holy See', 'Honduras', 'Hungary', 'Iceland',\n",
    "                                   'Iraq', 'Ireland', 'Israel', 'Italy',\n",
    "       'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya',\n",
    "       'Korea, South', 'Kosovo','Belgium', 'Belize', 'Benin', 'Bhutan',\n",
    "       'Bolivia', 'Bosnia and Herzegovina', 'Botswana',\n",
    "       'Brunei', 'Bulgaria', 'Burkina Faso', 'Burma', 'Burundi',\n",
    "       'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada',\n",
    "       'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia',\n",
    "       'Comoros', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica',\n",
    "       \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark',\n",
    "       'Diamond Princess', 'Djibouti', 'Luxembourg', 'MS Zaandam', 'Madagascar', 'Malawi',\n",
    "       'Malaysia', 'Maldives', 'Mali', 'Malta', 'Mauritania', 'Mauritius',\n",
    "       'Mexico', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco',\n",
    "       'Mozambique', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand',\n",
    "       'Nicaragua', 'Niger', 'Panama', 'Papua New Guinea', 'Paraguay',\n",
    "       'Peru', 'Philippines', 'Bahamas', 'Egypt'])\n",
    "    time_idx = [datetime.strptime(each, \"%m/%d/%y\") for each in sir_plot.date] #to convert all the dates into datetime \n",
    "    time_str= [each.strftime('%Y-%m-%d') for each in time_idx] #to convert datetime function to string\n",
    "    #time_str[0:5]\n",
    "    \n",
    "    #Storing the processed data file and sep';' is a seperator [German std]\n",
    "    sir_plot.to_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_sir_flat_table.csv', sep=';',index=False)\n",
    "    \n",
    "    \n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jTN5YIoimwK"
   },
   "source": [
    "## 3. Filter and Doubling Rate Calculation (Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sxIqbW4imwL",
    "outputId": "308ce079-7950-4109-c50f-954f47b82f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "             date state  COUNTRY   confirmed CODE  confirmed_filtered  \\\n",
      "130081 2022-06-26    no  Germany  27771911.0  DEU          27855312.8   \n",
      "130082 2022-06-27    no  Germany  27914240.0  DEU          27937262.8   \n",
      "130083 2022-06-28    no  Germany  28048190.0  DEU          28041832.4   \n",
      "130084 2022-06-29    no  Germany  28180861.0  DEU          28172904.3   \n",
      "130085 2022-06-30    no  Germany  28293960.0  DEU          28303976.2   \n",
      "\n",
      "        confirmed_DR  confirmed_filtered_DR  \n",
      "130081  69428.445000             392.941960  \n",
      "130082    388.730195             358.921129  \n",
      "130083    202.052613             299.644677  \n",
      "130084    210.394258             238.079171  \n",
      "130085    229.274012             214.942366  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','COUNTRY',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['state','COUNTRY']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter is a digital filter that can be applied to a set of digital data points for the purpose of \n",
    "        smoothing the data, that is, to increase the precision of the data without distorting the signal tendency.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression is used to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','COUNTRY',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','COUNTRY',filter_on]].groupby(['state','COUNTRY']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/20200823_COVID_relational_confirmed.csv',sep=';',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=pd_result_larg['confirmed']>100\n",
    "    pd_result_larg['confirmed_filtered_DR']=pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['COUNTRY']=='Germany'].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXL85sS9imwN"
   },
   "source": [
    "## 4. SIR Modelling \n",
    "\n",
    "Susceptible Infected and Recovered model is used to fit the Beta(Infection rate) and Gamma(recovery rate) for each country by using the available data and I have also generated the fitted curve to show how well the processed data fits over truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOpeh7_ximwO",
    "outputId": "c71f117e-67ac-469e-a8c1-431974942bf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-f2854a00e6a8>:46: RuntimeWarning: overflow encountered in double_scalars\n",
      "  dI_dt=beta*S*I/N0-gamma*I\n",
      "<ipython-input-5-f2854a00e6a8>:47: RuntimeWarning: overflow encountered in double_scalars\n",
      "  dR_dt=gamma*I\n",
      "C:\\Users\\donda\\anaconda3\\lib\\site-packages\\scipy\\integrate\\odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "<ipython-input-5-f2854a00e6a8>:45: RuntimeWarning: overflow encountered in double_scalars\n",
      "  dS_dt=-beta*S*I/N0          #S*I is the\n",
      "C:\\Users\\donda\\anaconda3\\lib\\site-packages\\scipy\\optimize\\minpack.py:828: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "<ipython-input-5-f2854a00e6a8>:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dI_dt=beta*S*I/N0-gamma*I\n",
      "C:\\Users\\donda\\anaconda3\\lib\\site-packages\\scipy\\integrate\\odepack.py:247: ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "\n",
    "df_input_large=pd.read_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_sir_flat_table.csv',sep=';').iloc[80:]\n",
    "pop = 100000000;\n",
    "df_all = df_input_large.columns\n",
    "df_all = list(df_all)\n",
    "\n",
    "def SIR_model(SIR,beta,gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "        \n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    S,I,R=SIR\n",
    "    dS_dt=-beta*S*I/N0          #S*I is the \n",
    "    dI_dt=beta*S*I/N0-gamma*I\n",
    "    dR_dt=gamma*I\n",
    "    return([dS_dt,dI_dt,dR_dt])\n",
    "\n",
    "\n",
    "# Functions for SIR model with time step\n",
    "def SIR_model_t(SIR,t,beta,gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        t: time step, mandatory for integral.odeint\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "        \n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    S,I,R=SIR\n",
    "    dS_dt=-beta*S*I/N0          #S*I is the \n",
    "    dI_dt=beta*S*I/N0-gamma*I\n",
    "    dR_dt=gamma*I\n",
    "    return dS_dt,dI_dt,dR_dt\n",
    "\n",
    "\n",
    "#Function defined for optimize curve fit\n",
    "def fit_odeint(x, beta, gamma):\n",
    "    '''\n",
    "    helper function for the integration\n",
    "    '''\n",
    "    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI\n",
    "\n",
    "#Fitting parameter for SIR model\n",
    "for each in df_all[1:]:\n",
    "    ydata = np.array(df_input_large[each])\n",
    "    t=np.arange(len(ydata))\n",
    "    N0 = 6000000 #max susceptible population\n",
    "\n",
    "    # ensure re-initialization \n",
    "    I0=ydata[0]\n",
    "    S0=N0-I0\n",
    "    R0=0\n",
    "\n",
    "    popt, pcov = optimize.curve_fit(fit_odeint, t, ydata, maxfev = 20000)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    # get the final fitted curve\n",
    "    fitted=fit_odeint(t, *popt).reshape(-1,1)\n",
    "    df_input_large[each +'_fitted'] = fitted \n",
    "    \n",
    "df_input_large.to_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_sir_fitted_table.csv', sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChJGCTSUimwR"
   },
   "source": [
    "## 5. Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donda\\anaconda3\\lib\\site-packages\\dash_bootstrap_components\\table.py:1: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n",
      "<ipython-input-1-20770e8aa0c4>:7: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donda\\Downloads\\Lecture_Covid_19_data_analysis-master (2)\\Lecture_Covid_19_data_analysis-master\\notebooks\n",
      "Dash is running on http://127.0.0.1:8000/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "import dash\n",
    "dash.__version__\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "df_input_large = pd.read_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_final_set.csv',sep=';')\n",
    "df = pd.read_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_CRD.csv',sep=';')\n",
    "df_input_sir = pd.read_csv('C:/Users/donda/Downloads/Lecture_Covid_19_data_analysis-master (2)/Lecture_Covid_19_data_analysis-master/data/processed/COVID_sir_fitted_table.csv',sep=';')\n",
    "df_all = df_input_sir.columns\n",
    "df_all = list(df_all[:109])\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.title = 'COVID-19 Dashboard'\n",
    "\n",
    "app.layout = html.Div([\n",
    "        \n",
    "        dbc.Row(dbc.Col(html.Div(dcc.Markdown('''\n",
    "                            # Enterprise Data Science: COVID-19 Data Analytics\n",
    "                            Goals of the Delivery 4:                            \n",
    "                            * Delivery 4 : SIR virus spread model is implemented in the dynamic dashboard\n",
    "                            \n",
    "                            ''')),\n",
    "                        width={'size': 10, 'offset': 1},\n",
    "                        )\n",
    "                ),\n",
    "        \n",
    "        dbc.Row(\n",
    "            [   \n",
    "                #Dropdown for SIR model\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                            id='country_dropdown_sir',\n",
    "                            options=[ {'label': each,'value':each} for each in df_all[1:]],\n",
    "                            value='India', # which are pre-selected\n",
    "                            multi= False\n",
    "                            ),\n",
    "                        width={'size': 5, \"offset\": 2, 'order': 'second'}\n",
    "                        ),\n",
    "                ], className=\"g-0\"\n",
    "        ),\n",
    "    \n",
    "        \n",
    "                \n",
    "        dbc.Row(\n",
    "            [\n",
    "                \n",
    "                \n",
    "                dbc.Col(dcc.Graph(\n",
    "                            id='SIR_model'\n",
    "                            ),\n",
    "                        width=6, md={'size': 5,  \"offset\": 1, 'order': 'last'}\n",
    "                        ),\n",
    "            ]\n",
    "        ),\n",
    "      \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('SIR_model', 'figure'),\n",
    "    [Input('country_dropdown_sir', 'value')])\n",
    "\n",
    "def SIR_fig(con_input):\n",
    "    df= df_input_sir\n",
    "   \n",
    "    \n",
    "    for i in df[1:]:\n",
    "        data = []\n",
    "        trace = go.Scatter(x=df.date,\n",
    "                        y=df[con_input],\n",
    "                        mode='lines+markers',\n",
    "                        name = con_input)\n",
    "        data.append(trace)\n",
    "        \n",
    "        trace_fitted = go.Scatter(x=df.date,\n",
    "                        y=df[con_input +'_fitted'], \n",
    "                        mode='lines+markers',\n",
    "                        name=con_input+'_fitted')\n",
    "        data.append(trace_fitted)\n",
    "        \n",
    "        \n",
    "            \n",
    "    return {'data': data,\n",
    "            'layout' : dict(\n",
    "                width=1280,\n",
    "                height=720,\n",
    "                title= 'SIR model',\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "                yaxis={'type':\"log\",\n",
    "                       'range':'[1.1,5.5]'\n",
    "                      }\n",
    "                \n",
    "            )\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, port=8000, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Evaluation_Walk_through_Vishal_Mukunda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
